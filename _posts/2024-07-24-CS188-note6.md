---
title: Carlo Tree Search,Probability Review, Bayesian Network Representation
author: Sillycheese
date: 2024-07-26 18:00:00 +0800
math: true
categories:
  - CS188
tags:
  - AI
---

## Monte Carlo Tree Search

### Two Ideas

- Evaluation by rollouts: play multiple games to termination from 
  a state s (using a simple, fast rollout policy) and count wins and 
  losses
- Selective search: explore parts of the tree that will help improve 
  the decision at the root, regardless of depth

## UCB algorithm

$$
\text{UCB1}(n) = \frac{U(n)}{N(n)} + C \times \sqrt{\frac{\log N(\text{PARENT}(n))}{N(n)}}
$$

- N(n): the total number of rollouts from node n, which captures *how promising*
- U(n): the total number of wins for Player(Parent(n)), which captures *how uncertain*
- C: user-specified parameter, balance the weight we put in the two terms

## Three steps for MCTS UCT tree search

1. The UCB criterion is used to move down the layers of a tree from the root node until an unexpanded leaf node is reached.
2. A new child is added to that leaf, and we run a rollout from that child to determine the number of wins from that node.
3. We update the numbers of wins from the child back up to the root node.
whose behavior is just like a minimax agent
## Probability Rundown

the **chain rule**

$$
P(A, B) = P(A|B)P(B) = P(B|A)P(A)
$$
$$
P(A1, A2...Ak) = P(A1)P(A2|A1)...P(Ak|A1...Ak−1)
$$

When A and B are mutually independent, P(A, B) = P(A)P(B).
## Probabilistic Inference

其实就是条件的各种推论，没什么好讲的
##  Inference By Enumeration

In Inference By Enumeration, we follow the following algorithm:
1. Collect all the rows consistent with the observed evidence variables.
2. Sum out (marginalize) all the hidden variables. 
3. Normalize the table so that it is a probability distribution (i.e. values sum to 1)
其实就是枚举法，看上去似乎和边缘分布也有些联系

## Bayesian Network Representation

使用无环图(DAG)来展示关系,相比于表格，能计算更大更复杂的联合分布
We formally define a Bayes Net as consisting of:
- A directed acyclic graph of nodes, one per variable X. 
- A conditional distribution for each node P(X|A1 . . . An), where Ai is the ith parent of X, stored as a **conditional probability table** or CPT. Each CPT has n + 2 columns: one for the values of each of the n parent variables A1 . . . An, one for the values of X, and one for the conditional probability of X given its parents.
$$
P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i \mid \text{parents}(X_i))
$$
## Structure of Bayes Nets
Two rules for Bayes Net independences
- Each node is **conditionally independent** of all its ancestor nodes (non-descendants) in the graph, given all of its parents.
- Each node is **conditionally independent** of all other variables given its **Markov blanket**.(A variable’s Markov blanket consists of parents, children, children’s other parents)
each node is conditionally independent of all its ancestor nodes in the graph, given all of its parents
$$
P(\text{var} \mid \text{Parents}(\text{var}), \text{Ancestors}(\text{var})) = P(\text{var} \mid \text{Parents}(\text{var}))
$$
## Exact Inference in Bayes Nets

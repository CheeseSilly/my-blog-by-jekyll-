---
title: Carlo Tree Search,Probability Review, Bayesian Network Representation
author: Sillycheese
date: 2024-07-26 18:00:00 +0800
math: true
categories:
  - CS188
tags:
  - AI
---

## Monte Carlo Tree Search

### Two Ideas

- Evaluation by rollouts: play multiple games to termination from 
  a state s (using a simple, fast rollout policy) and count wins and 
  losses
- Selective search: explore parts of the tree that will help improve 
  the decision at the root, regardless of depth

## UCB algorithm

$$
\text{UCB1}(n) = \frac{U(n)}{N(n)} + C \times \sqrt{\frac{\log N(\text{PARENT}(n))}{N(n)}}
$$

- N(n): the total number of rollouts from node n, which captures *how promising*
- U(n): the total number of wins for Player(Parent(n)), which captures *how uncertain*
- C: user-specified parameter, balance the weight we put in the two terms

## Three steps for MCTS UCT tree search

1. The UCB criterion is used to move down the layers of a tree from the root node until an unexpanded leaf node is reached.
2. A new child is added to that leaf, and we run a rollout from that child to determine the number of wins from that node.
3. We update the numbers of wins from the child back up to the root node.
whose behavior is just like a minimax agent
## Probability Rundown

the **chain rule**

$$
P(A, B) = P(A|B)P(B) = P(B|A)P(A)
$$
$$
P(A1, A2...Ak) = P(A1)P(A2|A1)...P(Ak|A1...Ak−1)
$$

When A and B are mutually independent, P(A, B) = P(A)P(B).
## Probabilistic Inference

其实就是条件的各种推论，没什么好讲的
##  Inference By Enumeration

In Inference By Enumeration, we follow the following algorithm:
1. Collect all the rows consistent with the observed evidence variables.
2. Sum out (marginalize) all the hidden variables. 
3. Normalize the table so that it is a probability distribution (i.e. values sum to 1)
其实就是枚举法，看上去似乎和边缘分布也有些联系

## Bayesian Network Representation

使用无环图(DAG)来展示关系,相比于表格，能计算更大更复杂的联合分布
We formally define a Bayes Net as consisting of:
- A directed acyclic graph of nodes, one per variable X. 
- A conditional distribution for each node P(X|A1 . . . An), where Ai is the ith parent of X, stored as a **conditional probability table** or CPT. Each CPT has n + 2 columns: one for the values of each of the n parent variables A1 . . . An, one for the values of X, and one for the conditional probability of X given its parents.
$$
P(X_1, X_2, \ldots, X_n) = \prod_{i=1}^{n} P(X_i \mid \text{parents}(X_i))
$$
## Structure of Bayes Nets
Two rules for Bayes Net independences
- Each node is **conditionally independent** of all its ancestor nodes (non-descendants) in the graph, given all of its parents.
- Each node is **conditionally independent** of all other variables given its **Markov blanket**.(A variable’s Markov blanket consists of parents, children, children’s other parents)
each node is conditionally independent of all its ancestor nodes in the graph, given all of its parents
$$
P(\text{var} \mid \text{Parents}(\text{var}), \text{Ancestors}(\text{var})) = P(\text{var} \mid \text{Parents}(\text{var}))
$$
## Exact Inference in Bayes Nets

### Variable Elimination

通过消除一个又一个的变量来求出目标变量的边缘分布

## Approximate Inference in Bayes Nets: Sampling

### Prior Sampling

从根节点开始，再从子节点开始。对每个结点计算并获取样本

### Rejection Sampling

从候选样本中通过特定条件来决定是否接受该样本

### Likelihood Weighting

对每一个样本设定一个权重

sampling a value if the variable is not an evidence variable, or changing the weight for the sample if the variable is evidence.

For example, suppose we want to calculate P(T | + c, +e). For the jth sample, we’d perform the following algorithm: 
- Set w j to 1.0, and c = true and e = true. 
- For T : This is not an evidence variable, so we sample t j from P(T ).
- For C: This is an evidence variable, so we multiply the weight of the sample by P(+c|t j), i.e. w j = w j · P(+c|t j). 
- For S: sample s j from P(S|t j). 
- For E: multiply the weight of the sample by P(+e| + c, s j), i.e. w j = w j · P(+e| + c, s j).

$$
\begin{equation}
P(z_1, z_2, \ldots, z_p, e_1, e_2, \ldots, e_m) = \prod_{i=1}^{p} P(z_i \mid \text{Parents}(z_i)) \cdot \prod_{i=1}^{m} P(e_i \mid \text{Parents}(e_i))
\end{equation}
$$

### Gibbs Sampling

让我们先来个Overview

In this approach, we first set all variables to some totally random value (not taking into account any CPTs). We then repeatedly pick one variable at a time, clear its value, and resample it given the values currently assigned to all other variables.

简单来讲就是对每一个变量依次进行条件采样，逐步构建整个系统的样本。


